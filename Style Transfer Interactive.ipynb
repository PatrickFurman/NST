{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6576ae3a",
   "metadata": {},
   "source": [
    "## Options\n",
    "#### Color Preservation:\n",
    "- No preservation (this will allow the style image to override the content image's color)\n",
    "- Lumiance synthesis (this will maintain the content image's color with a focus on maintaining accurate colors)\n",
    "- Color synthesis (this will maintain the content image's color with a focus on maintaining accurate texture from the style image)\n",
    "\n",
    "#### Style weighting\n",
    "- Adjust the ratio to determine how heavily the algorithm should weigh maintaining the content of the content image vs incorporating more of the style image\n",
    "\n",
    "#### Masked style transfer\n",
    "- Choose between applying style to background, foreground, or both\n",
    "\n",
    "#### Automatic upsampling\n",
    "- Automatically apply upsampling algorithm after generated the new image. Useful if you're going to print or display at a higher resolution anyways, but may reduce quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b5908bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG19, vgg19\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import util\n",
    "import IPython.display as display\n",
    "import PIL.Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26a3ba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Settings\n",
    "# Color preservation\n",
    "color_preserve = False\n",
    "\n",
    "# Style weighting\n",
    "style_weight = 8e-5\n",
    "\n",
    "# Masking\n",
    "mask = None\n",
    "\n",
    "# Upsampling\n",
    "upsample = False\n",
    "\n",
    "# Starting Image (content, style, random)\n",
    "starter_image = 'random'\n",
    "\n",
    "# Image paths\n",
    "style_image_path = './Style images/shipwreck 2.jpg'\n",
    "content_image_path = './Content images/mom.jpg'\n",
    "mask_path = './Mask images/mom mask.jpg'\n",
    "\n",
    "# RGB Images\n",
    "b, g, r = cv2.split(cv2.imread(style_image_path))\n",
    "style_image = cv2.merge((r, g, b))\n",
    "b, g, r = cv2.split(cv2.imread(content_image_path))\n",
    "content_image = cv2.merge((r, g, b))\n",
    "\n",
    "b, g, r = cv2.split(cv2.imread(mask_path))\n",
    "mask_image = cv2.merge((r, g, b))\n",
    "mask_image = tf.where((tf.image.rgb_to_grayscale(mask_image) != 255), 1, 0)\n",
    "\n",
    "# Display selected images\n",
    "window_name = 'test_view'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "029b4435",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = 500 # Largest number of pixels for longest side of image\n",
    "largest_dim = np.argmax(content_image.shape)\n",
    "if content_image.shape[largest_dim] > max_size:\n",
    "    # Rescale content image\n",
    "    curr_size = content_image.shape[largest_dim]\n",
    "    scale_ratio = curr_size / max_size\n",
    "    new_dims = [int(content_image.shape[0] / scale_ratio), int(content_image.shape[1] / scale_ratio)]\n",
    "    content_image = np.array(tf.image.resize(content_image, new_dims, preserve_aspect_ratio=True), dtype=np.uint8)\n",
    "    \n",
    "    if mask_path:\n",
    "        # Rescale mask image\n",
    "        mask = tf.Variable(tf.image.resize(mask_image, new_dims, preserve_aspect_ratio=True))[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf243ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]], shape=(500, 375), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[ 2.5082187e-04  2.1170831e-04  2.0137939e-04]\n",
      "  [ 2.4693375e-03  2.6559313e-03  2.6231294e-03]\n",
      "  [ 3.5637739e-04 -5.8971928e-04  1.6507764e-04]\n",
      "  ...\n",
      "  [ 3.9118258e-05 -6.0485024e-04 -3.0342329e-05]\n",
      "  [ 1.3496235e-04 -4.4515100e-04  3.6013712e-06]\n",
      "  [ 1.1623058e-04 -1.8395182e-04  3.0859897e-05]]\n",
      "\n",
      " [[-2.3813157e-04 -5.7723618e-04 -4.1522831e-04]\n",
      "  [ 2.6404136e-03  2.5273254e-03  2.6610638e-03]\n",
      "  [-1.5341262e-03 -3.6069597e-03 -2.2925185e-03]\n",
      "  ...\n",
      "  [ 2.3564060e-04 -7.4398040e-04  3.5195152e-04]\n",
      "  [ 2.4127909e-04 -6.8801898e-04  1.6183048e-04]\n",
      "  [ 1.0562186e-04 -4.2726865e-04 -6.5979507e-06]]\n",
      "\n",
      " [[-1.9194557e-04 -5.7436310e-04 -3.9164542e-04]\n",
      "  [ 3.5474810e-03  3.6927522e-03  3.6597839e-03]\n",
      "  [ 1.3972468e-03  4.5355748e-05  7.5699372e-04]\n",
      "  ...\n",
      "  [ 6.9051073e-04  8.4234052e-05  1.2540604e-03]\n",
      "  [ 7.3309150e-04  6.6683089e-05  9.5588359e-04]\n",
      "  [ 1.7695116e-04 -3.0063841e-04  1.5098268e-04]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 4.4004139e-04  1.0073830e-03  6.2859699e-04]\n",
      "  [ 6.9877348e-04  1.4216591e-03  8.5841009e-04]\n",
      "  [ 1.9906291e-04  3.0697972e-04 -1.4680890e-04]\n",
      "  ...\n",
      "  [-1.3838933e-04  2.7455066e-04  1.7093041e-04]\n",
      "  [-8.4884385e-05  1.2072197e-04  6.8970352e-05]\n",
      "  [-7.3968098e-05  1.0888471e-05 -1.0592378e-05]]\n",
      "\n",
      " [[ 4.9169856e-04  1.1218688e-03  7.0562441e-04]\n",
      "  [ 2.3662945e-04  1.1374532e-03  4.4230782e-04]\n",
      "  [ 2.8873005e-04  8.0727885e-04  7.6493321e-05]\n",
      "  ...\n",
      "  [ 6.2255218e-05  4.7248529e-04  3.1820172e-04]\n",
      "  [-2.1462220e-05  1.6831780e-04  8.7083492e-05]\n",
      "  [-2.8519837e-05  4.6619509e-05  1.2859686e-05]]\n",
      "\n",
      " [[ 3.6255579e-04  7.5725821e-04  4.9403269e-04]\n",
      "  [ 3.8073293e-04  1.0209129e-03  5.4453348e-04]\n",
      "  [ 8.4153964e-04  1.3611178e-03  8.1688445e-04]\n",
      "  ...\n",
      "  [ 5.4056469e-05  3.0251552e-04  1.9168777e-04]\n",
      "  [-3.2210708e-05  7.7489953e-05  2.0528463e-05]\n",
      "  [-2.8781056e-05  1.2098325e-05 -1.2411822e-05]]], shape=(500, 375, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(mask)\n",
    "grad = train_step(image)[1]\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a9807cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_grad(grad, mask):\n",
    "    channels = tf.split(grad, 3, axis=2)\n",
    "    for i in range(len(channels)):\n",
    "        channels[i] = tf.expand_dims(tf.multiply(channels[i][:,:,0],mask), 2)\n",
    "    return tf.concat(channels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6876ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(500, 375), dtype=float32, numpy=\n",
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0206048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal: 18444.0\n",
      "Masked: 18444\n"
     ]
    }
   ],
   "source": [
    "print(\"Equal:\", np.sum(mask_grad(grad, mask) == grad))\n",
    "print(\"Masked:\", np.sum(mask == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd4190b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "if color_preserve:\n",
    "    # Adjust luminance of style image to better match content before transferring style\n",
    "    style_image = util.transfer_luminance(content_image, style_image)\n",
    "    \n",
    "# Adjust size of style image to match that of content image so they can both pass through the same network\n",
    "content_image_dimensions = content_image.shape\n",
    "height, width, channels = content_image_dimensions\n",
    "style_image = np.array(tf.image.resize(style_image, content_image_dimensions[:2]), dtype=np.uint8)\n",
    "\n",
    "#style_image = vgg19.preprocess_input(style_image)\n",
    "#content_image = vgg19.preprocess_input(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4d2855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r, g, b = cv2.split(content_image)\n",
    "#cv2.imshow(\"content\", cv2.merge((b, g, r)))\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()\n",
    "\n",
    "#r, g, b = cv2.split(style_image)\n",
    "#cv2.imshow('style', cv2.merge((b, g, r)))\n",
    "#cv2.waitKey(0)\n",
    "#cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcd5dcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://arxiv.org/pdf/1508.06576.pdf (pages 9 and 10)\n",
    "# Which layers are used for style loss\n",
    "s_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1', 'block5_conv1']\n",
    "\n",
    "# Which layers are used for content loss (try with 'conv5_2')\n",
    "c_layers = ['block4_conv2']\n",
    "\n",
    "### Loss functions\n",
    "def content_loss(content_features_dict, generated):\n",
    "    generated = generated['content']\n",
    "    l = tf.add_n([0.5*tf.reduce_sum(tf.square(tf.subtract(content_features_dict[name], generated[name])))]\n",
    "                      for name in content_features_dict.keys())\n",
    "    l = l / num_content_layers\n",
    "    return tf.cast(l, tf.float64)\n",
    "\n",
    "def style_loss(style_features_dict, generated):\n",
    "    generated = generated['style']\n",
    "    layer_losses = []\n",
    "    for layer in style_features_dict.keys():\n",
    "        dividend = tf.cast(tf.reduce_sum(tf.square(tf.subtract(style_features_dict[layer], generated[layer]))), tf.float64)\n",
    "        divisor = 4*(tf.cast(tf.math.square(generated[layer].shape[1]), tf.float64)**2)\n",
    "        layer_losses.append(dividend / divisor)\n",
    "    l = tf.add_n(layer_losses)\n",
    "    l = l / num_style_layers\n",
    "    return l\n",
    "\n",
    "def total_loss(content_features, style_features, generated):\n",
    "    l = content_weight*content_loss(content_features, generated) + style_weight*style_loss(style_features, generated)\n",
    "    return l\n",
    "\n",
    "### Old loss functions (unused)\n",
    "\n",
    "def style_loss_old(style_features_dict, generated):\n",
    "    generated = generated['style']\n",
    "    l = tf.math.add_n([tf.math.divide(tf.cast(tf.math.reduce_sum(tf.math.square(tf.math.subtract(style_features_dict[name], \n",
    "                                                                                         generated[name]))), tf.float64),\n",
    "                                     tf.cast(4*(tf.cast(tf.math.square(generated[name].shape[1]), tf.float64)**2), tf.float64))]\n",
    "                      for name in style_features_dict.keys())\n",
    "    l = l / num_style_layers\n",
    "    return l\n",
    "\n",
    "def style_content_loss(outputs):\n",
    "    style_outputs = outputs['style']\n",
    "    content_outputs = outputs['content']\n",
    "    style_loss = tf.add_n([tf.reduce_mean(abs((style_outputs[name]-style_targets[name]))) \n",
    "                           for name in style_outputs.keys()])\n",
    "    style_loss *= style_weight / num_style_layers\n",
    "\n",
    "    content_loss = tf.add_n([tf.reduce_mean(abs((content_outputs[name]-content_targets[name]))) \n",
    "                             for name in content_outputs.keys()])\n",
    "    content_loss *= content_weight / num_content_layers\n",
    "    loss = style_loss + content_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40a8ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network definition\n",
    "model = VGG19(include_top = False, pooling= 'max', weights = 'imagenet', input_shape = content_image_dimensions)\n",
    "class NSTModel(tf.keras.models.Model):\n",
    "  def __init__(self, style_layers, content_layers, model):\n",
    "    super(NSTModel, self).__init__()\n",
    "    self.style_layers = style_layers\n",
    "    self.content_layers = content_layers\n",
    "    self.num_style_layers = len(style_layers)\n",
    "    outputs = [model.get_layer(name).output for name in s_layers+c_layers]\n",
    "    model = tf.keras.Model([model.input], outputs)\n",
    "    self.model = model\n",
    "    self.model.trainable = False\n",
    "\n",
    "  def call(self, inputs):\n",
    "    \"Expects float input in [0,1]\"\n",
    "    #inputs = inputs*255.0\n",
    "    #preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
    "    preprocessed_input = tf.reshape(inputs, (1, height, width, channels))\n",
    "    outputs = self.model(preprocessed_input)\n",
    "    style_outputs, content_outputs = (outputs[:self.num_style_layers],\n",
    "                                      outputs[self.num_style_layers:])\n",
    "\n",
    "    style_outputs = [util.gram_matrix(style_output)\n",
    "                     for style_output in style_outputs]\n",
    "\n",
    "    content_dict = {content_name: value\n",
    "                    for content_name, value\n",
    "                    in zip(self.content_layers, content_outputs)}\n",
    "\n",
    "    style_dict = {style_name: value\n",
    "                  for style_name, value\n",
    "                  in zip(self.style_layers, style_outputs)}\n",
    "\n",
    "    return {'content': content_dict, 'style': style_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b68e6df",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "extractor = NSTModel(s_layers, c_layers, model)\n",
    "\n",
    "white_noise = np.random.uniform(size=content_image_dimensions)\n",
    "style_image = util.scale_image(tf.Variable(tf.cast(tf.convert_to_tensor(style_image), dtype=tf.float32), trainable=True))\n",
    "content_image = util.scale_image(tf.Variable(tf.cast(tf.convert_to_tensor(content_image), dtype=tf.float32), trainable=True))\n",
    "image = tf.Variable(tf.cast(tf.convert_to_tensor(style_image), dtype=tf.float32), trainable=True)\n",
    "\n",
    "# style_grams = [util.gram_matrix(elem) for elem in extractor(style_image)['style'].values()]\n",
    "# style_targets = dict(zip(s_layers, style_grams))\n",
    "style_targets = extractor(style_image)['style']\n",
    "content_targets = extractor(content_image)['content']\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=1500,\n",
    "    decay_rate=0.95)\n",
    "opt = tf.optimizers.Adam(learning_rate=0.001, epsilon=0.1)\n",
    "\n",
    "ratio = 1e-2 # Equal to alpha/beta (content/style) in paper - larger number emphasizes content more\n",
    "content_weight = 1e-3\n",
    "style_weight = content_weight / ratio\n",
    "total_variation_weight = 8e-4\n",
    "num_style_layers = len(s_layers)\n",
    "num_content_layers = len(c_layers)\n",
    "\n",
    "@tf.function()\n",
    "def train_step(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = extractor(image)\n",
    "        loss = total_loss(content_targets, style_targets, outputs)\n",
    "        #loss += total_variation_weight*tf.cast(tf.image.total_variation(image), tf.float64)\n",
    "        grad = tape.gradient(loss, image)\n",
    "        if loss < 30000000:\n",
    "            grad = tf.clip_by_norm(grad, 5)\n",
    "        opt.apply_gradients([(grad, image)])\n",
    "    image.assign(util.scale_image(image))\n",
    "    return(loss, grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285ec95",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "epochs = 200\n",
    "steps_per_epoch = 50\n",
    "step_list = np.linspace(start=steps_per_epoch, stop=steps_per_epoch*epochs, num=epochs, dtype=np.int32)\n",
    "loss_list = []\n",
    "\n",
    "step = 0\n",
    "for n in range(epochs):\n",
    "    for m in range(steps_per_epoch):\n",
    "        step += 1\n",
    "        l = train_step(image)\n",
    "        print(\".\", end='', flush=True)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(PIL.Image.fromarray(np.array(image*255, dtype=np.uint8)))\n",
    "    loss_list.append(l)\n",
    "    loss_diff = l - loss_list[len(loss_list)-2]\n",
    "    print(\"Train step: %d   Total Loss %d   Change in Loss %d\"%(step, l, loss_diff))\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time: {:.1f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34afa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.random.choice([True, False], size=[450, 600])\n",
    "tensor = tf.Variable(train_step(image)[1])\n",
    "#tensor[:,:,0]\n",
    "tf.multiply(tensor[:,:,2],mask)\n",
    "for i in range(3):\n",
    "    tensor[:,:,i].assign(tf.multiply(tensor[:,:,i],mask))\n",
    "tensor\n",
    "#np.sum(mask==0)\n",
    "#tf.boolean_mask(tensor[:,:,0], mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0031af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.concat(tf.split(tensor, 3, 2), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd96d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Content loss: %f\" % content_loss(content_targets, extractor(image)), \n",
    "      \"\\nStyle Loss: %f\" % style_loss(style_targets, extractor(image)),\n",
    "      \"\\nTotal Variation: %f\" % (tf.image.total_variation(image)*total_variation_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Content loss: %f\" % content_loss(content_targets, extractor(image)), \n",
    "      \"\\nStyle Loss: %f\" % style_loss(style_targets, extractor(image)),\n",
    "      \"\\nTotal Variation: %f\" % (tf.image.total_variation(image)*total_variation_weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce58994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as tape:\n",
    "    outputs = extractor(image)\n",
    "    loss = total_loss(content_targets, style_targets, outputs)\n",
    "    grad = tf.clip_by_norm(tape.gradient(loss, image), 1)\n",
    "    print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae896ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r, g, b = cv2.split(np.array(image*255, dtype=np.uint8))\n",
    "#bgr_stylized = cv2.merge((b, g, r))\n",
    "#cv2.imwrite('stylized.png', bgr_stylized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
